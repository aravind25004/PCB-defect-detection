{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport numpy as np\n\n# Set seed for reproducibility\ntorch.manual_seed(42)\n\nclass ContinuousAtrousConvModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ContinuousAtrousConvModule, self).__init__()\n        # Three parallel atrous convolutions with different rates [1, 2, 4]\n        middle_channels = out_channels // 3\n        self.atrous_conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size=3, dilation=1, padding=1)\n        self.atrous_conv2 = nn.Conv2d(in_channels, middle_channels, kernel_size=3, dilation=2, padding=2)\n        self.atrous_conv3 = nn.Conv2d(in_channels, out_channels - 2*middle_channels, kernel_size=3, dilation=4, padding=4)\n        self.relu = nn.ReLU(inplace=True)\n        self.bn = nn.BatchNorm2d(out_channels)\n        \n        # Global average pooling to get 1×1×2048\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n    def forward(self, x):\n        x1 = self.atrous_conv1(x)\n        x2 = self.atrous_conv2(x)\n        x3 = self.atrous_conv3(x)\n        x = torch.cat([x1, x2, x3], dim=1)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.global_pool(x)  # Global average pooling to get 1×1×2048\n        return x\n\nclass ImprovedSkipLayer(nn.Module):\n    def __init__(self, low_level_channels, mid_level_channels, high_level_channels, num_classes):\n        super(ImprovedSkipLayer, self).__init__()\n        \n        # Convolutional layers for skip connections\n        self.conv_low = nn.Conv2d(low_level_channels, num_classes, kernel_size=1)\n        self.conv_mid = nn.Conv2d(mid_level_channels, num_classes, kernel_size=1)\n        self.conv_high = nn.Conv2d(high_level_channels, num_classes, kernel_size=1)\n        \n    def forward(self, x_low, x_mid, x_high, x_upsampled):\n        # Process each feature level\n        x_low_processed = self.conv_low(x_low)\n        x_mid_processed = self.conv_mid(x_mid)\n        x_high_processed = self.conv_high(x_high)\n        \n        # 8x upsampling from 1×1×4 to match high level features\n        x_upsampled_8x = F.interpolate(x_upsampled, size=x_high_processed.shape[2:], \n                                      mode='bilinear', align_corners=False)\n        \n        # Fusion with high-level features\n        high_fusion = x_high_processed + x_upsampled_8x\n        \n        # 4x upsampling to match mid level features\n        high_fusion_upsampled = F.interpolate(high_fusion, size=x_mid_processed.shape[2:], \n                                             mode='bilinear', align_corners=False)\n        \n        # Fusion with mid-level features\n        mid_fusion = x_mid_processed + high_fusion_upsampled\n        \n        # 2x upsampling to match low level features\n        mid_fusion_upsampled = F.interpolate(mid_fusion, size=x_low_processed.shape[2:], \n                                           mode='bilinear', align_corners=False)\n        \n        # Fusion with low-level features\n        final_fusion = x_low_processed + mid_fusion_upsampled\n        \n        # Final upsampling to original image size\n        output = F.interpolate(final_fusion, size=(640, 640), mode='bilinear', align_corners=False)\n        \n        return output\n\nclass InvertedResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, expansion_factor=6):\n        super(InvertedResidualBlock, self).__init__()\n        self.stride = stride\n        self.use_residual = stride == 1 and in_channels == out_channels\n        \n        hidden_dim = in_channels * expansion_factor\n        \n        # 1x1 pointwise conv for expansion\n        self.expand = nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True)\n        ) if expansion_factor != 1 else nn.Identity()\n        \n        # 3x3 depthwise conv\n        self.depthwise = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, \n                     groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True)\n        )\n        \n        # 1x1 pointwise conv for projection\n        self.project = nn.Sequential(\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n    \n    def forward(self, x):\n        identity = x\n        \n        x = self.expand(x)\n        x = self.depthwise(x)\n        x = self.project(x)\n        \n        if self.use_residual:\n            x = x + identity\n        \n        return x\n\nclass ImprovedFCN(nn.Module):\n    def __init__(self, num_classes=6):  # Set default to 4 classes as per paper\n        super(ImprovedFCN, self).__init__()\n        \n        # Use MobileNetV2 as backbone (as described in the paper)\n        self.mobilenet = models.mobilenet_v2(pretrained=True)\n        \n        # Extract feature maps at different levels\n        # We'll need to track the layers to extract low, mid and high level features\n        \n        # MobileNet feature extractor until different stages\n        self.low_level_features = nn.Sequential(*list(self.mobilenet.features)[:4])    # Earlier layers\n        self.mid_level_features = nn.Sequential(*list(self.mobilenet.features)[4:7])   # Middle layers\n        self.high_level_features = nn.Sequential(*list(self.mobilenet.features)[7:14]) # Later layers\n        self.final_features = nn.Sequential(*list(self.mobilenet.features)[14:])       # Final layers\n        \n        # Channel dimensions at each level (based on MobileNetV2 architecture)\n        self.low_level_channels = 24    # Adjust based on actual channel numbers\n        self.mid_level_channels = 32\n        self.high_level_channels = 96\n        self.final_channels = 1280\n        \n        # Continuous Atrous Convolution Module\n        self.continuous_atrous = ContinuousAtrousConvModule(self.final_channels, 2048)\n        \n        # Final prediction layer\n        self.final_conv = nn.Conv2d(2048, num_classes, kernel_size=1)\n        \n        # Improved Skip Layer\n        self.improved_skip = ImprovedSkipLayer(\n            self.low_level_channels, \n            self.mid_level_channels, \n            self.high_level_channels, \n            num_classes\n        )\n        \n        # Softmax classifier\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        input_size = x.size()[2:]  # Remember original input size\n        \n        # Extract features at different levels\n        x_low = self.low_level_features(x)\n        print(f\"Low level features shape: {x_low.shape}\")\n        \n        x = x_low\n        x_mid = self.mid_level_features(x)\n        print(f\"Mid level features shape: {x_mid.shape}\")\n        \n        x = x_mid\n        x_high = self.high_level_features(x)\n        print(f\"High level features shape: {x_high.shape}\")\n        \n        x = x_high\n        x = self.final_features(x)\n        print(f\"Final features shape: {x.shape}\")\n        \n        # Apply Continuous Atrous Convolution Module\n        x = self.continuous_atrous(x)\n        print(f\"After continuous atrous: {x.shape}\")\n        \n        # Final prediction\n        x = self.final_conv(x)\n        print(f\"After final conv: {x.shape}\")\n        \n        # Apply improved skip connections and upsampling\n        x = self.improved_skip(x_low, x_mid, x_high, x)\n        print(f\"After improved skip layer: {x.shape}\")\n        \n        # Softmax classifier\n        x = self.softmax(x)\n        \n        return x\n\n# Test the model with dummy input\nif __name__ == \"__main__\":\n    # Create dummy input\n    dummy_input = torch.randn(1, 3, 640, 640)\n    \n    # Initialize model\n    model = ImprovedFCN(num_classes=6)  # 4 defect types as per paper\n    model.eval()\n    \n    # Run forward pass\n    with torch.no_grad():\n        output = model(dummy_input)\n        \n    print(f\"Output shape: {output.shape}\")  # Should be [1, 4, 640, 640]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:40.952456Z","iopub.execute_input":"2025-04-13T17:00:40.952763Z","iopub.status.idle":"2025-04-13T17:00:41.646403Z","shell.execute_reply.started":"2025-04-13T17:00:40.952740Z","shell.execute_reply":"2025-04-13T17:00:41.645607Z"}},"outputs":[{"name":"stdout","text":"Low level features shape: torch.Size([1, 24, 160, 160])\nMid level features shape: torch.Size([1, 32, 80, 80])\nHigh level features shape: torch.Size([1, 96, 40, 40])\nFinal features shape: torch.Size([1, 1280, 20, 20])\nAfter continuous atrous: torch.Size([1, 2048, 1, 1])\nAfter final conv: torch.Size([1, 6, 1, 1])\nAfter improved skip layer: torch.Size([1, 6, 640, 640])\nOutput shape: torch.Size([1, 6, 640, 640])\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport numpy as np\n\n# Set seed for reproducibility\ntorch.manual_seed(42)\n\nclass ContinuousAtrousConvModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ContinuousAtrousConvModule, self).__init__()\n        # Three parallel atrous convolutions with different rates [1, 2, 4]\n        middle_channels = out_channels // 3\n        self.atrous_conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size=3, dilation=1, padding=1)\n        self.atrous_conv2 = nn.Conv2d(in_channels, middle_channels, kernel_size=3, dilation=2, padding=2)\n        self.atrous_conv3 = nn.Conv2d(in_channels, out_channels - 2*middle_channels, kernel_size=3, dilation=4, padding=4)\n        self.relu = nn.ReLU(inplace=True)\n        self.bn = nn.BatchNorm2d(out_channels)\n        \n        # Global average pooling to get 1×1×2048\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n    def forward(self, x):\n        x1 = self.atrous_conv1(x)\n        x2 = self.atrous_conv2(x)\n        x3 = self.atrous_conv3(x)\n        x = torch.cat([x1, x2, x3], dim=1)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.global_pool(x)  # Global average pooling to get 1×1×2048\n        return x\n\nclass ImprovedSkipLayer(nn.Module):\n    def __init__(self, low_level_channels, mid_level_channels, high_level_channels, num_classes):\n        super(ImprovedSkipLayer, self).__init__()\n        \n        # Convolutional layers for skip connections\n        self.conv_low = nn.Conv2d(low_level_channels, num_classes, kernel_size=1)  # 160×160×128 -> 160×160×num_classes\n        self.conv_mid = nn.Conv2d(mid_level_channels, num_classes, kernel_size=1)  # 80×80×256 -> 80×80×num_classes  \n        self.conv_high = nn.Conv2d(high_level_channels, num_classes, kernel_size=1) # 40×40×512 -> 40×40×num_classes\n        \n    def forward(self, x_low, x_mid, x_high, x_upsampled):\n        # Process each feature level\n        x_low_processed = self.conv_low(x_low)\n        x_mid_processed = self.conv_mid(x_mid)\n        x_high_processed = self.conv_high(x_high)\n        \n        # 8x upsampling from 1×1×num_classes to match high level features\n        x_upsampled_8x = F.interpolate(x_upsampled, size=x_high_processed.shape[2:], \n                                      mode='bilinear', align_corners=False)\n        \n        # Fusion with high-level features\n        high_fusion = x_high_processed + x_upsampled_8x\n        \n        # 4x upsampling to match mid level features\n        high_fusion_upsampled = F.interpolate(high_fusion, size=x_mid_processed.shape[2:], \n                                             mode='bilinear', align_corners=False)\n        \n        # Fusion with mid-level features\n        mid_fusion = x_mid_processed + high_fusion_upsampled\n        \n        # 2x upsampling to match low level features\n        mid_fusion_upsampled = F.interpolate(mid_fusion, size=x_low_processed.shape[2:], \n                                           mode='bilinear', align_corners=False)\n        \n        # Fusion with low-level features\n        final_fusion = x_low_processed + mid_fusion_upsampled\n        \n        # Final upsampling to original image size (from low level to original size)\n        output = F.interpolate(final_fusion, size=(640, 640), mode='bilinear', align_corners=False)\n        \n        return output\n\nclass InvertedResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, expansion_factor=6):\n        super(InvertedResidualBlock, self).__init__()\n        self.stride = stride\n        self.use_residual = stride == 1 and in_channels == out_channels\n        \n        hidden_dim = in_channels * expansion_factor\n        \n        # 1x1 pointwise conv for expansion\n        self.expand = nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True)\n        ) if expansion_factor != 1 else nn.Identity()\n        \n        # 3x3 depthwise conv\n        self.depthwise = nn.Sequential(\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, \n                     groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True)\n        )\n        \n        # 1x1 pointwise conv for projection\n        self.project = nn.Sequential(\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n    \n    def forward(self, x):\n        identity = x\n        \n        x = self.expand(x)\n        x = self.depthwise(x)\n        x = self.project(x)\n        \n        if self.use_residual:\n            x = x + identity\n        \n        return x\n\nclass ImprovedFCN(nn.Module):\n    def __init__(self, num_classes=6):  # 6 classes as per your implementation\n        super(ImprovedFCN, self).__init__()\n        \n        # Use MobileNetV2 as backbone (as described in the paper)\n        self.mobilenet = models.mobilenet_v2(pretrained=True)\n        \n        # Extract feature maps at different levels\n        # MobileNet feature extractor until different stages\n        self.low_level_features = nn.Sequential(*list(self.mobilenet.features)[:4])    # Earlier layers\n        self.mid_level_features = nn.Sequential(*list(self.mobilenet.features)[4:7])   # Middle layers\n        self.high_level_features = nn.Sequential(*list(self.mobilenet.features)[7:14]) # Later layers\n        self.final_features = nn.Sequential(*list(self.mobilenet.features)[14:])       # Final layers\n        \n        # Channel dimensions at each level (based on MobileNetV2 architecture)\n        self.low_level_channels = 24    # Actual channel number in MobileNetV2\n        self.mid_level_channels = 32    # Actual channel number in MobileNetV2\n        self.high_level_channels = 96    # Actual channel number in MobileNetV2\n        self.final_channels = 1280       # Actual channel number in MobileNetV2\n        \n        # Continuous Atrous Convolution Module\n        self.continuous_atrous = ContinuousAtrousConvModule(self.final_channels, 2048)\n        \n        # Final prediction layer\n        self.final_conv = nn.Conv2d(2048, num_classes, kernel_size=1)\n        \n        # Improved Skip Layer\n        self.improved_skip = ImprovedSkipLayer(\n            self.low_level_channels, \n            self.mid_level_channels, \n            self.high_level_channels, \n            num_classes\n        )\n        \n        # Softmax classifier\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        # Original input size: 640×640×3\n        print(f\"Input: {x.shape}\")\n        \n        # Extract features at different levels with MobileNetV2\n        x_low = self.low_level_features(x)\n        print(f\"Low level features shape: {x_low.shape}\")  # Should be around 160×160×24\n        \n        x_mid = self.mid_level_features(x_low)\n        print(f\"Mid level features shape: {x_mid.shape}\")  # Should be around 80×80×32\n        \n        x_high = self.high_level_features(x_mid)\n        print(f\"High level features shape: {x_high.shape}\")  # Should be around 40×40×96\n        \n        x_final = self.final_features(x_high)\n        print(f\"Final features shape: {x_final.shape}\")  # Should be around 20×20×1280\n        \n        # Apply Continuous Atrous Convolution Module\n        x_atrous = self.continuous_atrous(x_final)\n        print(f\"After continuous atrous: {x_atrous.shape}\")  # Should be 1×1×2048\n        \n        # Final prediction\n        x_pred = self.final_conv(x_atrous)\n        print(f\"After final conv: {x_pred.shape}\")  # Should be 1×1×num_classes\n        \n        # Apply improved skip connections and upsampling\n        output = self.improved_skip(x_low, x_mid, x_high, x_pred)\n        print(f\"After improved skip layer: {output.shape}\")  # Should be 640×640×num_classes\n        \n        # Softmax classifier\n        output = self.softmax(output)\n        print(f\"Final output shape: {output.shape}\")  # Should be 640×640×num_classes\n        \n        return output\n\n# Test the model with dummy input\nif __name__ == \"__main__\":\n    # Create dummy input\n    dummy_input = torch.randn(1, 3, 640, 640)\n    \n    # Initialize model\n    model = ImprovedFCN(num_classes=6)  # As per your implementation\n    model.eval()\n    \n    # Run forward pass\n    with torch.no_grad():\n        output = model(dummy_input)\n        \n    print(f\"Output shape: {output.shape}\")  # Should be [1, 6, 640, 640]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:13:16.012009Z","iopub.execute_input":"2025-04-13T17:13:16.012316Z","iopub.status.idle":"2025-04-13T17:13:16.628119Z","shell.execute_reply.started":"2025-04-13T17:13:16.012292Z","shell.execute_reply":"2025-04-13T17:13:16.627365Z"}},"outputs":[{"name":"stdout","text":"Input: torch.Size([1, 3, 640, 640])\nLow level features shape: torch.Size([1, 24, 160, 160])\nMid level features shape: torch.Size([1, 32, 80, 80])\nHigh level features shape: torch.Size([1, 96, 40, 40])\nFinal features shape: torch.Size([1, 1280, 20, 20])\nAfter continuous atrous: torch.Size([1, 2048, 1, 1])\nAfter final conv: torch.Size([1, 6, 1, 1])\nAfter improved skip layer: torch.Size([1, 6, 640, 640])\nFinal output shape: torch.Size([1, 6, 640, 640])\nOutput shape: torch.Size([1, 6, 640, 640])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"**Mobilenet v2**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torchvision.models import mobilenet_v2\n\n# Set seed for reproducibility\ntorch.manual_seed(42)\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        # Expansion phase (1x1 conv to increase channels)\n        if expand_ratio != 1:\n            layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n            layers.append(nn.BatchNorm2d(hidden_dim))\n            layers.append(nn.ReLU6(inplace=True))\n        \n        # Depthwise convolution\n        layers.extend([\n            # Depthwise convolution (3x3 with groups equal to input channels)\n            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True),\n        ])\n        \n        # Projection phase (1x1 conv to decrease channels)\n        layers.extend([\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n        ])\n        \n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\nclass ContinuousAtrousConvModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ContinuousAtrousConvModule, self).__init__()\n        # Three parallel atrous convolutions with different rates [1, 2, 4]\n        middle_channels = out_channels // 3\n        self.atrous_conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size=3, dilation=1, padding=1)\n        self.atrous_conv2 = nn.Conv2d(in_channels, middle_channels, kernel_size=3, dilation=2, padding=2)\n        self.atrous_conv3 = nn.Conv2d(in_channels, out_channels - 2*middle_channels, kernel_size=3, dilation=4, padding=4)\n        self.relu = nn.ReLU(inplace=True)\n        self.bn = nn.BatchNorm2d(out_channels)\n        \n        # Global average pooling to get 1×1×out_channels\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n    def forward(self, x):\n        print(f\"ContinuousAtrousConvModule input: {x.shape}\")\n        x1 = self.atrous_conv1(x)\n        x2 = self.atrous_conv2(x)\n        x3 = self.atrous_conv3(x)\n        x = torch.cat([x1, x2, x3], dim=1)\n        print(f\"After atrous convolutions and concatenation: {x.shape}\")\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.global_pool(x)\n        print(f\"After global pooling: {x.shape}\")\n        return x\n\nclass ImprovedSkipLayer(nn.Module):\n    def __init__(self, low_level_channels, mid_level_channels, high_level_channels, num_classes):\n        super(ImprovedSkipLayer, self).__init__()\n        \n        # Convolutional layers for skip connections\n        self.conv_low = nn.Conv2d(low_level_channels, num_classes, kernel_size=1)\n        self.conv_mid = nn.Conv2d(mid_level_channels, num_classes, kernel_size=1)\n        self.conv_high = nn.Conv2d(high_level_channels, num_classes, kernel_size=1)\n        \n    def forward(self, x_low, x_mid, x_high, x_upsampled):\n        print(f\"ImprovedSkipLayer inputs - low: {x_low.shape}, mid: {x_mid.shape}, high: {x_high.shape}, upsampled: {x_upsampled.shape}\")\n        \n        # Process each feature level\n        x_low_processed = self.conv_low(x_low)\n        x_mid_processed = self.conv_mid(x_mid)\n        x_high_processed = self.conv_high(x_high)\n        \n        print(f\"After 1x1 convolutions - low: {x_low_processed.shape}, mid: {x_mid_processed.shape}, high: {x_high_processed.shape}\")\n        \n        # 8x upsampling from 1×1×num_classes to match high-level features\n        x_upsampled_8x = F.interpolate(x_upsampled, size=x_high_processed.shape[2:], \n                                      mode='bilinear', align_corners=False)\n        print(f\"After 8x upsampling: {x_upsampled_8x.shape}\")\n        \n        # Fusion with high-level features\n        high_fusion = x_high_processed + x_upsampled_8x\n        print(f\"After high fusion: {high_fusion.shape}\")\n        \n        # 2x upsampling (from high to mid)\n        high_fusion_upsampled = F.interpolate(high_fusion, size=x_mid_processed.shape[2:], \n                                            mode='bilinear', align_corners=False)\n        print(f\"After high to mid upsampling: {high_fusion_upsampled.shape}\")\n        \n        # Fusion with mid-level features\n        mid_fusion = x_mid_processed + high_fusion_upsampled\n        print(f\"After mid fusion: {mid_fusion.shape}\")\n        \n        # 2x upsampling (from mid to low)\n        mid_fusion_upsampled = F.interpolate(mid_fusion, size=x_low_processed.shape[2:], \n                                           mode='bilinear', align_corners=False)\n        print(f\"After mid to low upsampling: {mid_fusion_upsampled.shape}\")\n        \n        # Fusion with low-level features\n        final_fusion = x_low_processed + mid_fusion_upsampled\n        print(f\"After final fusion: {final_fusion.shape}\")\n        \n        # Final 4x upsampling to original image size\n        output = F.interpolate(final_fusion, size=(640, 640), mode='bilinear', align_corners=False)\n        print(f\"Final output: {output.shape}\")\n        \n        return output\n\nclass ImprovedFCN_MobileNetV2(nn.Module):\n    def __init__(self, num_classes=6):\n        super(ImprovedFCN_MobileNetV2, self).__init__()\n        \n        # Load pre-trained MobileNetV2 as the backbone\n        mobilenet = mobilenet_v2(pretrained=True)\n        \n        # Extract low, mid, and high-level features from MobileNetV2\n        # Low-level features (after the first inverted residual block)\n        self.low_level_features = nn.Sequential(\n            mobilenet.features[0],  # Conv2d + BN + ReLU6\n            mobilenet.features[1],  # InvertedResidual (t=1, c=16, n=1, s=1)\n        )\n        \n        # Mid-level features (after the 3rd inverted residual block)\n        self.mid_level_features = nn.Sequential(\n            mobilenet.features[2],  # InvertedResidual (t=6, c=24, n=2, s=2)\n            mobilenet.features[3],  # InvertedResidual\n        )\n        \n        # High-level features (after the 6th inverted residual block)\n        self.high_level_features = nn.Sequential(\n            mobilenet.features[4],  # InvertedResidual (t=6, c=32, n=3, s=2)\n            mobilenet.features[5],  # InvertedResidual\n            mobilenet.features[6],  # InvertedResidual\n            mobilenet.features[7],  # InvertedResidual (t=6, c=64, n=4, s=2)\n            mobilenet.features[8],  # InvertedResidual\n            mobilenet.features[9],  # InvertedResidual\n            mobilenet.features[10], # InvertedResidual\n        )\n        \n        # Deeper features for continuous atrous convolution\n        self.deeper_features = nn.Sequential(\n            mobilenet.features[11],  # InvertedResidual (t=6, c=96, n=3, s=1)\n            mobilenet.features[12],  # InvertedResidual\n            mobilenet.features[13],  # InvertedResidual\n            mobilenet.features[14],  # InvertedResidual (t=6, c=160, n=3, s=2)\n            mobilenet.features[15],  # InvertedResidual\n            mobilenet.features[16],  # InvertedResidual\n            mobilenet.features[17],  # InvertedResidual (t=6, c=320, n=1, s=1)\n        )\n        \n        # Feature channels based on MobileNetV2 architecture\n        low_level_channels = 16    # After first inverted residual\n        mid_level_channels = 24    # After third inverted residual\n        high_level_channels = 64   # After seventh inverted residual\n        deeper_features_channels = 320  # After the final inverted residual\n\n        \n        \n        # Continuous Atrous Convolution Module\n        self.continuous_atrous = ContinuousAtrousConvModule(deeper_features_channels, 2048)\n        \n        # Final prediction layer\n        self.final_conv = nn.Conv2d(2048, num_classes, kernel_size=1)\n        \n        # Improved Skip Layer\n        self.improved_skip = ImprovedSkipLayer(low_level_channels, mid_level_channels, high_level_channels, num_classes)\n        \n        # Softmax classifier\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        # Initial shape should be B x 3 x 640 x 640\n        print(f\"Input: {x.shape}\")\n        \n        # Extract features at different levels from MobileNetV2\n        x_low = self.low_level_features(x)  # Low-level features\n        print(f\"Low-level features: {x_low.shape}\")\n        \n        x_mid = self.mid_level_features(x_low)  # Mid-level features\n        print(f\"Mid-level features: {x_mid.shape}\")\n        \n        x_high = self.high_level_features(x_mid)  # High-level features\n        print(f\"High-level features: {x_high.shape}\")\n        \n        x_deep = self.deeper_features(x_high)  # Deeper features for atrous\n        print(f\"Deeper features: {x_deep.shape}\")\n        \n        # Continuous Atrous Convolution Module\n        x = self.continuous_atrous(x_deep)\n        print(f\"After continuous atrous: {x.shape}\")\n        \n        # Final convolution for prediction\n        x = self.final_conv(x)\n        print(f\"After final_conv: {x.shape}\")\n        \n        # Apply improved skip connections and upsampling\n        x = self.improved_skip(x_low, x_mid, x_high, x)\n        print(f\"After improved skip layer: {x.shape}\")\n        \n        # Softmax classifier\n        x = self.softmax(x)\n        print(f\"After softmax: {x.shape}\")\n        \n        return x\n\n# Create dummy input (numpy to tensor)\ndummy_input_np = np.random.rand(1, 3, 640, 640).astype(np.float32)\ndummy_input_tensor = torch.from_numpy(dummy_input_np)\n\n# Initialize model\nmodel = ImprovedFCN_MobileNetV2(num_classes=6)\nmodel.eval()  # Set to eval mode\n\n# Run forward pass\nwith torch.no_grad():\n    output = model(dummy_input_tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T18:02:09.458078Z","iopub.execute_input":"2025-04-13T18:02:09.458383Z","iopub.status.idle":"2025-04-13T18:02:09.880802Z","shell.execute_reply.started":"2025-04-13T18:02:09.458358Z","shell.execute_reply":"2025-04-13T18:02:09.879979Z"}},"outputs":[{"name":"stdout","text":"Input: torch.Size([1, 3, 640, 640])\nLow-level features: torch.Size([1, 16, 320, 320])\nMid-level features: torch.Size([1, 24, 160, 160])\nHigh-level features: torch.Size([1, 64, 40, 40])\nDeeper features: torch.Size([1, 320, 20, 20])\nContinuousAtrousConvModule input: torch.Size([1, 320, 20, 20])\nAfter atrous convolutions and concatenation: torch.Size([1, 2048, 20, 20])\nAfter global pooling: torch.Size([1, 2048, 1, 1])\nAfter continuous atrous: torch.Size([1, 2048, 1, 1])\nAfter final_conv: torch.Size([1, 6, 1, 1])\nImprovedSkipLayer inputs - low: torch.Size([1, 16, 320, 320]), mid: torch.Size([1, 24, 160, 160]), high: torch.Size([1, 64, 40, 40]), upsampled: torch.Size([1, 6, 1, 1])\nAfter 1x1 convolutions - low: torch.Size([1, 6, 320, 320]), mid: torch.Size([1, 6, 160, 160]), high: torch.Size([1, 6, 40, 40])\nAfter 8x upsampling: torch.Size([1, 6, 40, 40])\nAfter high fusion: torch.Size([1, 6, 40, 40])\nAfter high to mid upsampling: torch.Size([1, 6, 160, 160])\nAfter mid fusion: torch.Size([1, 6, 160, 160])\nAfter mid to low upsampling: torch.Size([1, 6, 320, 320])\nAfter final fusion: torch.Size([1, 6, 320, 320])\nFinal output: torch.Size([1, 6, 640, 640])\nAfter improved skip layer: torch.Size([1, 6, 640, 640])\nAfter softmax: torch.Size([1, 6, 640, 640])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"**Mobilenet v2 final**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torchvision.models import mobilenet_v2\n\n# Set seed for reproducibility\ntorch.manual_seed(42)\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        # Expansion phase (1x1 conv to increase channels)\n        if expand_ratio != 1:\n            layers.append(nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False))\n            layers.append(nn.BatchNorm2d(hidden_dim))\n            layers.append(nn.ReLU6(inplace=True))\n        \n        # Depthwise convolution\n        layers.extend([\n            # Depthwise convolution (3x3 with groups equal to input channels)\n            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(inplace=True),\n        ])\n        \n        # Projection phase (1x1 conv to decrease channels)\n        layers.extend([\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n        ])\n        \n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\nclass ContinuousAtrousConvModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ContinuousAtrousConvModule, self).__init__()\n        # Three parallel atrous convolutions with different rates [1, 2, 4]\n        middle_channels = out_channels // 3\n        self.atrous_conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size=3, dilation=1, padding=1)\n        self.atrous_conv2 = nn.Conv2d(in_channels, middle_channels, kernel_size=3, dilation=2, padding=2)\n        self.atrous_conv3 = nn.Conv2d(in_channels, out_channels - 2*middle_channels, kernel_size=3, dilation=4, padding=4)\n        self.relu = nn.ReLU(inplace=True)\n        self.bn = nn.BatchNorm2d(out_channels)\n        \n        # Global average pooling to get 1×1×out_channels\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n    def forward(self, x):\n        print(f\"ContinuousAtrousConvModule input: {x.shape}\")\n        x1 = self.atrous_conv1(x)\n        x2 = self.atrous_conv2(x)\n        x3 = self.atrous_conv3(x)\n        x = torch.cat([x1, x2, x3], dim=1)\n        print(f\"After atrous convolutions and concatenation: {x.shape}\")\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.global_pool(x)\n        print(f\"After global pooling: {x.shape}\")\n        return x\n\nclass ImprovedSkipLayer(nn.Module):\n    def __init__(self, low_level_channels, mid_level_channels, high_level_channels, num_classes):\n        super(ImprovedSkipLayer, self).__init__()\n        \n        # Convolutional layers for skip connections\n        self.conv_low = nn.Conv2d(low_level_channels, num_classes, kernel_size=1)\n        self.conv_mid = nn.Conv2d(mid_level_channels, num_classes, kernel_size=1)\n        self.conv_high = nn.Conv2d(high_level_channels, num_classes, kernel_size=1)\n        \n    def forward(self, x_low, x_mid, x_high, x_upsampled):\n        print(f\"ImprovedSkipLayer inputs - low: {x_low.shape}, mid: {x_mid.shape}, high: {x_high.shape}, upsampled: {x_upsampled.shape}\")\n        \n        # Process each feature level\n        x_low_processed = self.conv_low(x_low)\n        x_mid_processed = self.conv_mid(x_mid)\n        x_high_processed = self.conv_high(x_high)\n        \n        print(f\"After 1x1 convolutions - low: {x_low_processed.shape}, mid: {x_mid_processed.shape}, high: {x_high_processed.shape}\")\n        \n        # 8x upsampling from 1×1×num_classes to match high-level features\n        x_upsampled_8x = F.interpolate(x_upsampled, size=x_high_processed.shape[2:], \n                                      mode='bilinear', align_corners=False)\n        print(f\"After 8x upsampling: {x_upsampled_8x.shape}\")\n        \n        # Fusion with high-level features\n        high_fusion = x_high_processed + x_upsampled_8x\n        print(f\"After high fusion: {high_fusion.shape}\")\n        \n        # 2x upsampling (from high to mid)\n        high_fusion_upsampled = F.interpolate(high_fusion, size=x_mid_processed.shape[2:], \n                                            mode='bilinear', align_corners=False)\n        print(f\"After high to mid upsampling: {high_fusion_upsampled.shape}\")\n        \n        # Fusion with mid-level features\n        mid_fusion = x_mid_processed + high_fusion_upsampled\n        print(f\"After mid fusion: {mid_fusion.shape}\")\n        \n        # 2x upsampling (from mid to low)\n        mid_fusion_upsampled = F.interpolate(mid_fusion, size=x_low_processed.shape[2:], \n                                           mode='bilinear', align_corners=False)\n        print(f\"After mid to low upsampling: {mid_fusion_upsampled.shape}\")\n        \n        # Fusion with low-level features\n        final_fusion = x_low_processed + mid_fusion_upsampled\n        print(f\"After final fusion: {final_fusion.shape}\")\n        \n        # Final 4x upsampling to original image size\n        output = F.interpolate(final_fusion, size=(640, 640), mode='bilinear', align_corners=False)\n        print(f\"Final output: {output.shape}\")\n        \n        return output\n\nclass ImprovedFCN_MobileNetV2(nn.Module):\n    def __init__(self, num_classes=6):\n        super(ImprovedFCN_MobileNetV2, self).__init__()\n        \n        # Load pre-trained MobileNetV2 as the backbone\n        mobilenet = mobilenet_v2(pretrained=True)\n        \n        # Extract low, mid, and high-level features from MobileNetV2\n        # Low-level features (after the first inverted residual block)\n        self.low_level_features = nn.Sequential(\n            mobilenet.features[0],  # Conv2d + BN + ReLU6\n            mobilenet.features[1],  # InvertedResidual (t=1, c=16, n=1, s=1)\n        )\n        \n        # Mid-level features (after the 3rd inverted residual block)\n        self.mid_level_features = nn.Sequential(\n            mobilenet.features[2],  # InvertedResidual (t=6, c=24, n=2, s=2)\n            mobilenet.features[3],  # InvertedResidual\n        )\n        \n        # High-level features (after the 6th inverted residual block)\n        self.high_level_features = nn.Sequential(\n            mobilenet.features[4],  # InvertedResidual (t=6, c=32, n=3, s=2)\n            mobilenet.features[5],  # InvertedResidual\n            mobilenet.features[6],  # InvertedResidual\n            mobilenet.features[7],  # InvertedResidual (t=6, c=64, n=4, s=2)\n            mobilenet.features[8],  # InvertedResidual\n            mobilenet.features[9],  # InvertedResidual\n            mobilenet.features[10], # InvertedResidual\n        )\n        \n        # Deeper features for continuous atrous convolution\n        self.deeper_features = nn.Sequential(\n            mobilenet.features[11],  # InvertedResidual (t=6, c=96, n=3, s=1)\n            mobilenet.features[12],  # InvertedResidual\n            mobilenet.features[13],  # InvertedResidual\n            mobilenet.features[14],  # InvertedResidual (t=6, c=160, n=3, s=2)\n            mobilenet.features[15],  # InvertedResidual\n            mobilenet.features[16],  # InvertedResidual\n            mobilenet.features[17],  # InvertedResidual (t=6, c=320, n=1, s=1)\n        )\n        \n        # Feature channels based on MobileNetV2 architecture\n        low_level_channels = 16    # After first inverted residual\n        mid_level_channels = 24    # After third inverted residual\n        high_level_channels = 64   # After seventh inverted residual\n        deeper_features_channels = 320  # After the final inverted residual\n        \n        # Add channel expansion layers before continuous atrous module\n        self.channel_expansion = nn.Conv2d(320, 1024, kernel_size=1, bias=False)\n        self.bn_expansion = nn.BatchNorm2d(1024)\n        self.relu_expansion = nn.ReLU6(inplace=True)\n        \n        # Continuous Atrous Convolution Module\n        self.continuous_atrous = ContinuousAtrousConvModule(1024, 2048)\n        \n        # Final prediction layer\n        self.final_conv = nn.Conv2d(2048, num_classes, kernel_size=1)\n        \n        # Improved Skip Layer\n        self.improved_skip = ImprovedSkipLayer(low_level_channels, mid_level_channels, high_level_channels, num_classes)\n        \n        # Softmax classifier\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        # Initial shape should be B x 3 x 640 x 640\n        print(f\"Input: {x.shape}\")\n        \n        # Extract features at different levels from MobileNetV2\n        x_low = self.low_level_features(x)  # Low-level features\n        print(f\"Low-level features: {x_low.shape}\")\n        \n        x_mid = self.mid_level_features(x_low)  # Mid-level features\n        print(f\"Mid-level features: {x_mid.shape}\")\n        \n        x_high = self.high_level_features(x_mid)  # High-level features\n        print(f\"High-level features: {x_high.shape}\")\n        \n        x_deep = self.deeper_features(x_high)  # Deeper features for atrous\n        print(f\"Deeper features: {x_deep.shape}\")\n        \n        # Apply channel expansion\n        x_expanded = self.relu_expansion(self.bn_expansion(self.channel_expansion(x_deep)))\n        print(f\"After channel expansion: {x_expanded.shape}\")\n        \n        # Continuous Atrous Convolution Module\n        x = self.continuous_atrous(x_expanded)\n        print(f\"After continuous atrous: {x.shape}\")\n        \n        # Final convolution for prediction\n        x = self.final_conv(x)\n        print(f\"After final_conv: {x.shape}\")\n        \n        # Apply improved skip connections and upsampling\n        x = self.improved_skip(x_low, x_mid, x_high, x)\n        print(f\"After improved skip layer: {x.shape}\")\n        \n        # Softmax classifier\n        x = self.softmax(x)\n        print(f\"After softmax: {x.shape}\")\n        \n        return x\n\n# Create dummy input (numpy to tensor)\ndummy_input_np = np.random.rand(1, 3, 640, 640).astype(np.float32)\ndummy_input_tensor = torch.from_numpy(dummy_input_np)\n\n# Initialize model\nmodel = ImprovedFCN_MobileNetV2(num_classes=6)\nmodel.eval()  # Set to eval mode\n\n# Run forward pass\nwith torch.no_grad():\n    output = model(dummy_input_tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T18:11:20.276877Z","iopub.execute_input":"2025-04-13T18:11:20.277205Z","iopub.status.idle":"2025-04-13T18:11:20.896542Z","shell.execute_reply.started":"2025-04-13T18:11:20.277180Z","shell.execute_reply":"2025-04-13T18:11:20.895673Z"}},"outputs":[{"name":"stdout","text":"Input: torch.Size([1, 3, 640, 640])\nLow-level features: torch.Size([1, 16, 320, 320])\nMid-level features: torch.Size([1, 24, 160, 160])\nHigh-level features: torch.Size([1, 64, 40, 40])\nDeeper features: torch.Size([1, 320, 20, 20])\nAfter channel expansion: torch.Size([1, 1024, 20, 20])\nContinuousAtrousConvModule input: torch.Size([1, 1024, 20, 20])\nAfter atrous convolutions and concatenation: torch.Size([1, 2048, 20, 20])\nAfter global pooling: torch.Size([1, 2048, 1, 1])\nAfter continuous atrous: torch.Size([1, 2048, 1, 1])\nAfter final_conv: torch.Size([1, 6, 1, 1])\nImprovedSkipLayer inputs - low: torch.Size([1, 16, 320, 320]), mid: torch.Size([1, 24, 160, 160]), high: torch.Size([1, 64, 40, 40]), upsampled: torch.Size([1, 6, 1, 1])\nAfter 1x1 convolutions - low: torch.Size([1, 6, 320, 320]), mid: torch.Size([1, 6, 160, 160]), high: torch.Size([1, 6, 40, 40])\nAfter 8x upsampling: torch.Size([1, 6, 40, 40])\nAfter high fusion: torch.Size([1, 6, 40, 40])\nAfter high to mid upsampling: torch.Size([1, 6, 160, 160])\nAfter mid fusion: torch.Size([1, 6, 160, 160])\nAfter mid to low upsampling: torch.Size([1, 6, 320, 320])\nAfter final fusion: torch.Size([1, 6, 320, 320])\nFinal output: torch.Size([1, 6, 640, 640])\nAfter improved skip layer: torch.Size([1, 6, 640, 640])\nAfter softmax: torch.Size([1, 6, 640, 640])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}